{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1\n",
    "\n",
    "Trigram probability estimation:\n",
    "\n",
    "$$P(w_n| w_{1:n-1}) \\approx P(w_n | w_{n-2:n-1}) = \\frac{C(w_{n-2}\\;w_{n-1}\\;w_n)}{C(w_{n-2}\\;w_{n-1})}$$\n",
    "\n",
    "$$P(\\text{am | <s> I}) = 1/2 \\\\ \n",
    "P(\\text{Sam | I am}) = 1/2 \\\\ \n",
    "P(\\text{</s> | am Sam}) = 1/1 \\\\\n",
    "P(\\text{I | <s> Sam}) = 1/1$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P(\\text{I want chinese food}) &= P(I | <s>) P(want | I) P(chinese | want) P(food | chinese) P(</s> | food) \\\\\n",
    "    &= (.25)(.33)(.0065)(.52)(.68) \\\\\n",
    "    &\\approx 1.89618 \\times 10^{-4}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P(\\text{i | <s>}) &= 0.19, P(\\text{</s> | food}) = 0.40 \\\\\n",
    "    P(\\text{I want chinese food}) &= (.19)(.21)(.0029)(.052)(.40)\\\\\n",
    "    &\\approx 2.42969 \\times 10^{-6}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3\n",
    "\n",
    "Unsmoothed: Probability mass is removed from present words and given to irrelevant words in smoothed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12\n"
     ]
    }
   ],
   "source": [
    "def laplace_smoothing(corpus, curr, prefix):\n",
    "    # preprocessing\n",
    "    corpus = list(map(lambda s: s.lower(), corpus))\n",
    "    curr = curr.lower()\n",
    "    prefix = prefix.lower()\n",
    "    # generate vocab\n",
    "    vocab = {w.lower for l in [s.split() for s in corpus] for w in l}\n",
    "    V = len(vocab)\n",
    "    # calculate original count\n",
    "    bigram_count = 0\n",
    "    prefix_count = 0\n",
    "    for sent in corpus:\n",
    "        words = sent.split()\n",
    "        for ix, word in enumerate(words[:-1]):\n",
    "            if word == prefix:\n",
    "                prefix_count += 1\n",
    "                if words[ix+1] == curr:\n",
    "                    bigram_count += 1\n",
    "    # calculate the add-one smoothed bigram counts (Eq 3.24)\n",
    "    return (bigram_count + 1) / (prefix_count + V)\n",
    "\n",
    "corpus = [\n",
    "    '<s> I am Sam </s>',\n",
    "    '<s> Sam I am </s>',\n",
    "    '<s> I am Sam </s>',\n",
    "    '<s> I do not like green eggs and Sam </s>'\n",
    "]\n",
    "print(laplace_smoothing(corpus, 'Sam', 'am'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6\n",
    "\n",
    "$$P(w_3 | w_1w_2) = \\frac{C(w_1w_2w_3) + 1}{C(w_1w_2) + V}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7\n",
    "\n",
    "Note: probabilities should all be handled in log space :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41333333333333333\n"
     ]
    }
   ],
   "source": [
    "def get_unigram_mle(corpus, curr):\n",
    "    # preprocessing\n",
    "    corpus = list(map(lambda s: s.lower(), corpus))\n",
    "    curr = curr.lower()\n",
    "    # total num tokens\n",
    "    N = sum(len(sent.split()) for sent in corpus)\n",
    "    # count curr\n",
    "    curr_count = 0\n",
    "    for sent in corpus:\n",
    "        for word in sent.split():\n",
    "            if word == curr:\n",
    "                curr_count += 1\n",
    "    return curr_count / N\n",
    "\n",
    "def get_bigram_mle(corpus, curr, prev):\n",
    "    # preprocessing\n",
    "    corpus = list(map(lambda s: s.lower(), corpus))\n",
    "    curr = curr.lower()\n",
    "    prev = prev.lower()\n",
    "    # calculate original count\n",
    "    bigram_count = 0\n",
    "    prev_count = 0\n",
    "    for sent in corpus:\n",
    "        words = sent.split()\n",
    "        for ix, word in enumerate(words[:-1]):\n",
    "            if word == prev:\n",
    "                prev_count += 1\n",
    "                if words[ix+1] == curr:\n",
    "                    bigram_count += 1\n",
    "    # calculate the mle\n",
    "    return bigram_count / prev_count\n",
    "\n",
    "def get_interpolated_bigram_mle(corpus, curr, prev, lambda1, lambda2):\n",
    "    return (lambda1 * get_unigram_mle(corpus, curr) + \n",
    "            lambda2 * get_bigram_mle(corpus, curr, prev))\n",
    "corpus = [\n",
    "    '<s> I am Sam </s>',\n",
    "    '<s> Sam I am </s>',\n",
    "    '<s> I am Sam </s>',\n",
    "    '<s> I do not like green eggs and Sam </s>'\n",
    "]\n",
    "\n",
    "print(get_interpolated_bigram_mle(corpus, 'sam', 'am', 0.5, 0.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "def preprocess(filename):\n",
    "    # assumes lines in file have no trailing whitespace (except newlines)\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        # join at newlines\n",
    "        full_text = ' '.join(lines)\n",
    "        # split sentences by .\n",
    "        sentences = full_text.split('.')\n",
    "        # add start/end sentence tags \n",
    "        sentences = [f'<s> {sent} </s>'.lower() for sent in sentences]\n",
    "        return sentences\n",
    "\n",
    "def get_unigrams(sentences, return_count=False):\n",
    "    # get counts for all unigrams\n",
    "    count_dict = defaultdict(int)\n",
    "    total = 0\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        total += len(words)\n",
    "        for word in words:\n",
    "            count_dict[word] += 1\n",
    "    # create probability dict based on counts\n",
    "    prob_dict = defaultdict(float)\n",
    "    for word, count in count_dict.items():\n",
    "        prob_dict[word] = count / total\n",
    "    if return_count:\n",
    "        return count_dict\n",
    "    return prob_dict\n",
    "\n",
    "def get_bigrams(sentences, get_unigrams, return_count=False):\n",
    "    # get counts for all bigrams\n",
    "    count_dict = defaultdict(int)\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        for ix, word in enumerate(words[:-1]):\n",
    "            count_dict[(words[ix], words[ix+1])] += 1\n",
    "    # create probability dict based on counts\n",
    "    prob_dict = defaultdict(float)\n",
    "    unigram_count_dict = get_unigrams(sentences, return_count=True)\n",
    "    for bigram, count in count_dict.items():\n",
    "        prob_dict[bigram] = count / unigram_count_dict[bigram[0]]\n",
    "    if return_count:\n",
    "        return count_dict\n",
    "    return prob_dict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams: [('the', 0.05179982440737489), ('of', 0.04477611940298507), ('<s>', 0.029850746268656716), ('</s>', 0.029850746268656716), ('to', 0.024582967515364356), ('a', 0.02370500438981563), ('and', 0.021071115013169446), ('in', 0.02019315188762072), ('their', 0.013169446883230905), ('&', 0.013169446883230905)]\n",
      "1.0000000000000102\n",
      "bigrams: [(('act', 'of'), 1.0), (('poses', 'an'), 1.0), (('interesting', 'question'), 1.0), (('question', 'of'), 1.0), (('choose', 'to'), 1.0), (('apply', 'the'), 1.0), (('syntax,', 'and'), 1.0), (('novel', 'language,'), 1.0), (('exhibit', 'a'), 1.0), (('deeper', 'understanding'), 1.0)]\n",
      "430.9999999999992\n"
     ]
    }
   ],
   "source": [
    "sentences = preprocess(os.getcwd() + '/../misc/linguistics_paper1.txt')\n",
    "ug_prob = get_unigrams(sentences)\n",
    "bg_prob = get_bigrams(sentences, get_unigrams)\n",
    "print('unigrams:', sorted(ug_prob.items(), key=lambda t: t[1], reverse=True)[:10])\n",
    "print(sum(ug_prob.values()))\n",
    "print('bigrams:', sorted(bg_prob.items(), key=lambda t: t[1], reverse=True)[:10])\n",
    "print(sum(bg_prob.values()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram sent:\n",
      " <s> the adjectival lexicon (culbertson & adger tested 160 native english-speaking adults through an online crowd-sourcing service to a preference for preserving scope (culbertson & adger explore which of two modifiers </s>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Note that the total probability for any given prefix should be ~1\n",
    "def generate_random_sentence(bg_model):\n",
    "    sent_words = []\n",
    "    curr_word = None\n",
    "    while curr_word != '</s>':\n",
    "        # Initialize sentence\n",
    "        if curr_word is None:\n",
    "            curr_word = '<s>'\n",
    "        else:\n",
    "        # Else, generate next word\n",
    "            prob_tuples = [t for t in bg_model.items() if t[0][0] == curr_word]\n",
    "            bg_tups, probs = [], []\n",
    "            for t in prob_tuples:\n",
    "                bg_tups.append(t[0])\n",
    "                probs.append(t[1])\n",
    "            probs = [t[1] for t in prob_tuples]\n",
    "            prob_ix = np.random.choice(len(probs), p=probs)\n",
    "            curr_word = bg_tups[prob_ix][1]\n",
    "        sent_words.append(curr_word)\n",
    "    return ' '.join(sent_words)\n",
    "\n",
    "print('bigram sent:\\n', generate_random_sentence(bg_prob))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.681145747868608\n"
     ]
    }
   ],
   "source": [
    "# Assumes all bigrams in test_seq have nonzero probability in bg_model\n",
    "def get_perplexity(bg_model, test_seq):\n",
    "    # Preprocessing\n",
    "    words = [w.lower() for w in test_seq.split()]\n",
    "    # Calculate inner product\n",
    "    prod_accum = 1\n",
    "    for i in range(1, len(words)):\n",
    "        prod_accum *= 1 / bg_model[(words[i-1], words[i])]\n",
    "    return np.power(prod_accum, 1/len(words))\n",
    "\n",
    "print(get_perplexity(bg_prob, 'the study'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.12\n",
    "\n",
    "$$\n",
    "W = 0 0 0 0 0 3 0 0 0 0\n",
    "\\begin{align*}\n",
    "    \\text{perplexity}(W) &= \\sqrt[10]{\\Pi_{i=1}^10 \\frac{1}{(P(w_i))}} \\\\\n",
    "    &= \\sqrt[10]{(\\frac{100}{91})^9(\\frac{100}{1})} \\\\\n",
    "    &= 1.725\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
