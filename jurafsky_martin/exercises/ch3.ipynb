{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1\n",
    "\n",
    "Trigram probability estimation:\n",
    "\n",
    "$$P(w_n| w_{1:n-1}) \\approx P(w_n | w_{n-2:n-1}) = \\frac{C(w_{n-2}\\;w_{n-1}\\;w_n)}{C(w_{n-2}\\;w_{n-1})}$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12\n"
     ]
    }
   ],
   "source": [
    "def laplace_smoothing(corpus, curr, prefix):\n",
    "    # preprocessing\n",
    "    corpus = list(map(lambda s: s.lower(), corpus))\n",
    "    curr = curr.lower()\n",
    "    prefix = prefix.lower()\n",
    "    # generate vocab\n",
    "    vocab = {w.lower for l in [s.split() for s in corpus] for w in l}\n",
    "    V = len(vocab)\n",
    "    # calculate original count\n",
    "    bigram_count = 0\n",
    "    prefix_count = 0\n",
    "    for sent in corpus:\n",
    "        words = sent.split()\n",
    "        for ix, word in enumerate(words[:-1]):\n",
    "            if word == prefix:\n",
    "                prefix_count += 1\n",
    "                if words[ix+1] == curr:\n",
    "                    bigram_count += 1\n",
    "    # calculate the add-one smoothed bigram counts (Eq 3.24)\n",
    "    return (bigram_count + 1) / (prefix_count + V)\n",
    "\n",
    "corpus = [\n",
    "    '<s> I am Sam </s>',\n",
    "    '<s> Sam I am </s>',\n",
    "    '<s> I am Sam </s>',\n",
    "    '<s> I do not like green eggs and Sam </s>'\n",
    "]\n",
    "print(laplace_smoothing(corpus, 'Sam', 'am'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7\n",
    "\n",
    "Note: probabilities should all be handled in log space :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41333333333333333\n"
     ]
    }
   ],
   "source": [
    "def get_unigram_mle(corpus, curr):\n",
    "    # preprocessing\n",
    "    corpus = list(map(lambda s: s.lower(), corpus))\n",
    "    curr = curr.lower()\n",
    "    # total num tokens\n",
    "    N = sum(len(sent.split()) for sent in corpus)\n",
    "    # count curr\n",
    "    curr_count = 0\n",
    "    for sent in corpus:\n",
    "        for word in sent.split():\n",
    "            if word == curr:\n",
    "                curr_count += 1\n",
    "    return curr_count / N\n",
    "\n",
    "def get_bigram_mle(corpus, curr, prev):\n",
    "    # preprocessing\n",
    "    corpus = list(map(lambda s: s.lower(), corpus))\n",
    "    curr = curr.lower()\n",
    "    prev = prev.lower()\n",
    "    # calculate original count\n",
    "    bigram_count = 0\n",
    "    prev_count = 0\n",
    "    for sent in corpus:\n",
    "        words = sent.split()\n",
    "        for ix, word in enumerate(words[:-1]):\n",
    "            if word == prev:\n",
    "                prev_count += 1\n",
    "                if words[ix+1] == curr:\n",
    "                    bigram_count += 1\n",
    "    # calculate the mle\n",
    "    return bigram_count / prev_count\n",
    "\n",
    "def get_interpolated_bigram_mle(corpus, curr, prev, lambda1, lambda2):\n",
    "    return (lambda1 * get_unigram_mle(corpus, curr) + \n",
    "            lambda2 * get_bigram_mle(corpus, curr, prev))\n",
    "corpus = [\n",
    "    '<s> I am Sam </s>',\n",
    "    '<s> Sam I am </s>',\n",
    "    '<s> I am Sam </s>',\n",
    "    '<s> I do not like green eggs and Sam </s>'\n",
    "]\n",
    "\n",
    "print(get_interpolated_bigram_mle(corpus, 'sam', 'am', 0.5, 0.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "def preprocess(filename):\n",
    "    # assumes lines in file have no trailing whitespace (except newlines)\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        # join at newlines\n",
    "        full_text = ' '.join(lines)\n",
    "        # split sentences by .\n",
    "        sentences = full_text.split('.')\n",
    "        # add start/end sentence tags \n",
    "        sentences = [f'<s> {sent} </s>'.lower() for sent in sentences]\n",
    "        return sentences\n",
    "\n",
    "def get_unigrams(sentences, return_count=False):\n",
    "    # get counts for all unigrams\n",
    "    count_dict = defaultdict(int)\n",
    "    total = 0\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        total += len(words)\n",
    "        for word in words:\n",
    "            count_dict[word] += 1\n",
    "    # create probability dict based on counts\n",
    "    prob_dict = defaultdict(float)\n",
    "    for word, count in count_dict.items():\n",
    "        prob_dict[word] = count / total\n",
    "    if return_count:\n",
    "        return count_dict\n",
    "    return prob_dict\n",
    "\n",
    "def get_bigrams(sentences, get_unigrams, return_count=False):\n",
    "    # get counts for all bigrams\n",
    "    count_dict = defaultdict(int)\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        for ix, word in enumerate(words[:-1]):\n",
    "            count_dict[(words[ix], words[ix+1])] += 1\n",
    "    # create probability dict based on counts\n",
    "    prob_dict = defaultdict(float)\n",
    "    unigram_count_dict = get_unigrams(sentences, return_count=True)\n",
    "    for bigram, count in count_dict.items():\n",
    "        prob_dict[bigram] = count / unigram_count_dict[bigram[0]]\n",
    "    if return_count:\n",
    "        return count_dict\n",
    "    return prob_dict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams: [('the', 0.05179982440737489), ('of', 0.04477611940298507), ('<s>', 0.029850746268656716), ('</s>', 0.029850746268656716), ('to', 0.024582967515364356), ('a', 0.02370500438981563), ('and', 0.021071115013169446), ('in', 0.02019315188762072), ('their', 0.013169446883230905), ('&', 0.013169446883230905)]\n",
      "1.0000000000000102\n",
      "bigrams: [(('act', 'of'), 1.0), (('poses', 'an'), 1.0), (('interesting', 'question'), 1.0), (('question', 'of'), 1.0), (('choose', 'to'), 1.0), (('apply', 'the'), 1.0), (('syntax,', 'and'), 1.0), (('novel', 'language,'), 1.0), (('exhibit', 'a'), 1.0), (('deeper', 'understanding'), 1.0)]\n",
      "430.9999999999992\n"
     ]
    }
   ],
   "source": [
    "sentences = preprocess(os.getcwd() + '/../misc/linguistics_paper1.txt')\n",
    "ug_prob = get_unigrams(sentences)\n",
    "bg_prob = get_bigrams(sentences, get_unigrams)\n",
    "print('unigrams:', sorted(ug_prob.items(), key=lambda t: t[1], reverse=True)[:10])\n",
    "print(sum(ug_prob.values()))\n",
    "print('bigrams:', sorted(bg_prob.items(), key=lambda t: t[1], reverse=True)[:10])\n",
    "print(sum(bg_prob.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
